{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "032a22d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import struct\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3134adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the list of brokers in the cluster\n",
    "KAFKA_BOOTSTRAP_SERVERS = ['10.67.22.8:9092']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc27312",
   "metadata": {},
   "source": [
    "## Receive Large Files Directly (Not Recommended)\n",
    "\n",
    "In this first case we will receive directly the pair of 32 MB raw data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Kafka consumer instance\n",
    "raw_consumer = KafkaConsumer(\n",
    "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,  # list of Kafka brokers\n",
    "    max_partition_fetch_bytes=73400320,\n",
    "    fetch_max_bytes=73400320,\n",
    "    consumer_timeout_ms=1000000                   # maximum time to wait for a new message \n",
    "                                                # before stopping the consumer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e96588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all available topics on the kafka brokers\n",
    "raw_consumer.topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bcbc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subscribe to a topic\n",
    "raw_consumer.subscribe('raw_data')\n",
    "\n",
    "# check the active subscriptions\n",
    "raw_consumer.subscription()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123366aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the polling strategy for the consumer\n",
    "raw_consumer.poll(timeout_ms=0,         # do not enable dead-times before one poll to the next\n",
    "              max_records=None,     # do not limit the number of records to consume at once \n",
    "              update_offsets=True   # update the reading offsets on this topic\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9223965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rawmessage(msg):\n",
    "    #file_number = int.from_bytes(msg.value, \"big\")\n",
    "    var = struct.unpack('f'*(len(msg.value)//4), msg.value)\n",
    "    \n",
    "    #print(file_number)\n",
    "    print(var[0], var[10], var[8388608], var[8388618])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0519737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this consumer will keep polling for messages \n",
    "# until stopped by the user\n",
    "# (or reaches the consumer_timeout_ms, if specified)\n",
    "for message in raw_consumer:\n",
    "    process_rawmessage(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4864a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045f1fc",
   "metadata": {},
   "source": [
    "## Receive Data Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b03fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Kafka consumer instance\n",
    "chunk_consumer = KafkaConsumer(\n",
    "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,  # list of Kafka brokers\n",
    "    consumer_timeout_ms=10000                  # maximum time to wait for a new message \n",
    "                                                # before stopping the consumer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1d80cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk_data', 'data_file', 'my_awesome_topic', 'raw_data'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all available topics on the kafka brokers\n",
    "chunk_consumer.topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f9ea83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk_data'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subscribe to a topic\n",
    "chunk_consumer.subscribe('chunk_data')\n",
    "\n",
    "# check the active subscriptions\n",
    "chunk_consumer.subscription()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f9d9a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up the polling strategy for the consumer\n",
    "chunk_consumer.poll(timeout_ms=0,         # do not enable dead-times before one poll to the next\n",
    "                    max_records=None,     # do not limit the number of records to consume at once \n",
    "                    update_offsets=True   # update the reading offsets on this topic\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "901387ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunkmessage(msg, verbose=True):\n",
    "    file_number, bin_number = struct.unpack(\">hh\", msg.key)\n",
    "    #var = struct.unpack('f'*(len(msg.value)//4), msg.value)\n",
    "    \n",
    "    if verbose:\n",
    "        print(file_number, bin_number)\n",
    "    #print(var[0], var[10], var[8388608], var[8388618])\n",
    "    return file_number, bin_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a4dc435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this consumer will keep polling for messages \n",
    "# until stopped by the user\n",
    "# (or reaches the consumer_timeout_ms, if specified)\n",
    "n_bin = np.zeros(shape=(10, 2048))\n",
    "\n",
    "key_prev = 0\n",
    "for message in chunk_consumer:\n",
    "    key, n = process_chunkmessage(message, verbose=False)\n",
    "    \n",
    "    if(key_prev > key):\n",
    "        print(\"File sent not in order\")\n",
    "        \n",
    "    n_bin[key - 2][n] = n\n",
    "    key_prev = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75965486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for row in n_bin:\n",
    "    missing_elements = list(filter(lambda x: x not in row, range(0, 2048)))\n",
    "    print(missing_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d102e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b1a6c",
   "metadata": {},
   "source": [
    "### Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fdc224b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/14 10:36:48 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://10.67.22.8:7077\")\\\n",
    "    .appName(\"Spark structured streaming application\")\\\n",
    "    .config(\"spark.executor.memory\", \"512m\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4)\\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark/jars/spark-sql-kafka-0-10_2.12-3.4.1.jar\" + \",\" \n",
    "                        + \"/usr/local/spark/jars/kafka-clients-2.1.1.jar\") \\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06cef3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mapd-b-2023-gr14-1:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.67.22.8:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://10.67.22.8:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e1683a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:2.0.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "76f97e4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m inputDF \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka.bootstrap.servers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKAFKA_BOOTSTRAP_SERVERS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafkaConsumer.pollTimeoutMs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstartingOffsets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxOffsetsPerTrigger\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubscribe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m    \n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/readwriter.py:277\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
     ]
    }
   ],
   "source": [
    "inputDF = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option(\"kafkaConsumer.pollTimeoutMs\", 5000)\\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 2048)\\\n",
    "    .option(\"subscribe\", \"chunk_data\")\\\n",
    "    .load()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8bab71",
   "metadata": {},
   "source": [
    "## Receive S3 File Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80555cee",
   "metadata": {},
   "source": [
    "Let us create a new Kafka consumer that will listen to the \"data_file\" topic: it will thus receive information on the S3 buckets where the data files are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee01860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Kafka consumer instance\n",
    "consumer = KafkaConsumer(\n",
    "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,  # list of Kafka brokers\n",
    "    consumer_timeout_ms=10000                   # maximum time to wait for a new message \n",
    "                                                # before stopping the consumer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dbe7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all available topics on the kafka brokers\n",
    "consumer.topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39517d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subscribe to a topic\n",
    "consumer.subscribe('data_file')\n",
    "\n",
    "# check the active subscriptions\n",
    "consumer.subscription()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the polling strategy for the consumer\n",
    "consumer.poll(timeout_ms=0,         # do not enable dead-times before one poll to the next\n",
    "              max_records=None,     # do not limit the number of records to consume at once \n",
    "              update_offsets=True   # update the reading offsets on this topic\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48121d86",
   "metadata": {},
   "source": [
    "Each time a new message arrives, the value will be converted to a dictionary containing the bucket and file data names. Then *rdds containing the datasets will be created* * and optionally the data files in the bucket will be deleted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set s3 client\n",
    "s3_client = boto3.client('s3',\n",
    "                         endpoint_url='https://cloud-areapd.pd.infn.it:5210',\n",
    "                         aws_access_key_id='ec71c86cfc994f95b5a3a3a6d173bccc',     # DO NOT WRITE KEYS\n",
    "                         aws_secret_access_key='--------------------------------', # DIRECTLY ON FILE!!!\n",
    "                         verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3febd1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = \"/home/lupi/Project/LocalData/\"\n",
    "\n",
    "def process_message(message, cleanup=True):\n",
    "    val_dict = json.loads(message.value)\n",
    "    bucket = val_dict[\"bucket_name\"]\n",
    "    real_file = val_dict[\"real_file\"]\n",
    "    imag_file = val_dict[\"imag_file\"]\n",
    "    \n",
    "    \n",
    "    # ... get data from bucket\n",
    "    \n",
    "    print(real_file, imag_file)\n",
    "    #s3_client.download_file('quax', real_file, (download_path+real_file))\n",
    "    \n",
    "    if cleanup:\n",
    "        s3_client.delete_object(Bucket=bucket, Key=real_file)\n",
    "        s3_client.delete_object(Bucket=bucket, Key=imag_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this consumer will keep polling for messages \n",
    "# until stopped by the user\n",
    "# (or reaches the consumer_timeout_ms, if specified)\n",
    "for message in consumer:\n",
    "    process_message(message, False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
