{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8018f197",
   "metadata": {},
   "source": [
    "- Andrea De Vita\n",
    "- Enrico Lupi\n",
    "- Manfredi Miranda\n",
    "- Francesco Zane\n",
    "\n",
    "-----------------------\n",
    "\n",
    "# Streaming Processing of the QUAX Experiment Data for the Detection of Galactic Axions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b81bd98",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "The axion is a hypothetical particle introduced to solve the strong CP problem of Quantum Chromo Dynamics. It is speculated that axions may also constitute the dark matter (DM) content in our galaxy. The [QUAX](https://www.pd.infn.it/eng/quax/) (QUaerere AXions) experiment aims at detecting this particle by using a copper cavity immersed in a static magnetic field of 8.1 T, cooled down at a working temperature of about 150 mK.\n",
    "\n",
    "The goal of this project is to create a quasi real-time processing chain of the data produced by the QUAX experimental apparatus and a live monitoring system of the detector data, using [Apache Kafka](https://kafka.apache.org/) and [Apache Spark](https://spark.apache.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ac8d9",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction) <br>\n",
    "    1.1. [Experiment](#intro_experiment) <br>\n",
    "    1.2. [Data Structure](#intro_data_structure) <br>\n",
    "    1.3. [Cluster](#intro_cluster) <br>\n",
    "2. [Data Processing](#processing) <br>\n",
    "    2.1. [Pipeline Overview](#pipeline) <br>\n",
    "    2.2. [Kafka - Streaming Data](#kafka) <br> \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.1. [Kafka Topics](#kafka_topic) <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.2. [Data Pre-processing](#kafka_preprocessing) <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.3. [Producer](#kafka_producer) <br>\n",
    "    2.3. [Spark - Distributed Processing](#spark) <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3.1. [Spark Structured Streaming](#spark_streaming) <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3.2. [FFT and Averaging](#spark_fft) <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3.3. [Output Message](#spark_output) <br>\n",
    "    2.4. [Live Plot and Monitoring](#live_plot) <br>\n",
    "3. [Performance](#performance) <br>\n",
    "    3.1. [Kafka](#test_kafka) <br>\n",
    "    3.2. [Spark](#test_spark) <br>\n",
    "4. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398118cf",
   "metadata": {},
   "source": [
    "## 1. Introduction <a name=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa93b89",
   "metadata": {},
   "source": [
    "### 1.1. Experiment <a name=\"intro_experiment\"></a>\n",
    "\n",
    "\n",
    "![quax lab](Images\\labQuax.webp)\n",
    "\n",
    "The QUAX experiment aims at axion detection by using a copper cavity immersed in a static magnetic field of 8.1T, cooled down at a working temperature of about 150mK. The axion is expected to couple with the spin of the electron, interacting with the cavity and inducing a radio-frequency that can be sensed via a Josephson parametric amplifier. For a given configuration of the RF cavity, a scan of the phase of the electromagnetic field is performed to be able to possibly identify a localised excess, a hint of the coupling of an axion with the photon. \n",
    "\n",
    "The data acquisition system of the QUAX experiment generates two streams of digitized reading of the amplifiers, representing the real and imaginary components of the measured phase. To improve the signal over noise ratio, a QUAX data-taking run extends over a long time (up to weeks), repeating the scans over multiple times. Data are saved locally on the DAQ servers in the form of binary files, each corresponding to a multitude of continuous scans performed in the entire frequency range. A single pair of raw files is thus representative of only a few seconds of data taking, but are already including several (thousands) scans. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe23e5c",
   "metadata": {},
   "source": [
    "### 1.2. Data Structure <a name=\"intro_data_structure\"></a>\n",
    "\n",
    "The dataset is composed of 2 sets (named duck_i and duck_q respectively) of .dat binary files, each one comprised of a continuous series of ADC readings from the amplifier. Each ADC reading is written in the raw files as a 32 bit floating point value. The ADC readout frequency is 2 × 10<sup>6</sup> Hz (2 MegaSample per second, or 2MS/s), thus resulting in a raw data throughput of 128 Mbps (16 MB/s). During data taking the readouts are formatted in .dat file such that each file is comprised of 8193 × 2<sup>10</sup> samples. This results in producing a pair of .dat files (duck_i and duck_q) every 4.2 s.\n",
    "\n",
    "The dataset is provided on a cloud storage s3 bucket hosted on Cloud Veneto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf733f7",
   "metadata": {},
   "source": [
    "### 1.3. Cluster <a name=\"intro_cluster\"></a>\n",
    "\n",
    "This project has been done on a cluster composed by 4 virtual machines, each with 4 VCPUs with 25 GB disk space and 8 GB RAM each. The virtual machines are hosted on [CloudVeneto](https://cloudveneto.it/), an OpenStack-based cloud managed by University of Padova and INFN. Spark version 3.3.2 (using Scala version 2.12.15) and Kafka version 3.4.0 will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5355d",
   "metadata": {},
   "source": [
    "## 2. Data Processing <a name=\"processing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6d1aa",
   "metadata": {},
   "source": [
    "The processing of the raw data is comprised of two phases:\n",
    "1. Run a Fourier transform on each scan to move from the time domain to the frequency domain\n",
    "2. Average (in bins of frequency) all scans in a data-taking run, to extract a single frequency scan\n",
    " \n",
    "This procedure is highly parallelizable, and should be implemented in a quasi-online pipeline for two main reasons:\n",
    "1. Monitoring the scans during the data taking to promptly spot and identify possible issues in the detector setup or instabilities in the condition of the experiment\n",
    "2. Data is continuously produced with a very large rate, and the local storage provided by the DAQ server of the QUAX experiment is not really suited for large-volume and long-lasting datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93512045",
   "metadata": {},
   "source": [
    "### 2.1. Pipeline Overview <a name=\"pipeline\"></a>\n",
    "\n",
    "The data processing pipeline will be implemented as follows:\n",
    "- Each pair of files is unpacked according to their schema and split into scans.\n",
    "- Data is produced to a Kafka topic by a stream-emulator script every 5 seconds to simulate the fixed ADC scanning rate and the fixed size of files written to disk. \n",
    "- The processing of each file runs is performed in a distributed framework using pySpark: for each scan, a FFT is executed in parallel and the results of all FFTs are averaged.\n",
    "- The results are re-injected into a new Kafka topic hosted on the same brokers.\n",
    "- A final consumer performs the plotting, displaying live updates of the scans and continuously updating the entire \"run-wide\" scan using bokeh.\n",
    "\n",
    "The overall pipeline can be thus summarised as:\n",
    "![pipeline schema](Images\\Pipeline_Schema2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df89d6e",
   "metadata": {},
   "source": [
    "### 2.2. Kafka - Streaming Data <a name=\"kafka\"></a>\n",
    "\n",
    "Apache Kafka is an open-source distributed event streaming platform for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. As discussed previously, in this work it will be used to handle the live streaming of data from the DAQ servers all the way to the final live plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b255b",
   "metadata": {},
   "source": [
    "#### 2.2.1. Kafka Topics <a name=\"kafka_topic\"></a>\n",
    "\n",
    "The first step is to create a topic on the broker to hold the data from the DAQ. We create it with 4 separate partitions and no replication. The meaning of the name *chunk_data* will be made clear in the next section.\n",
    "\n",
    "We will also create a second topic for later, aptly named *results*, where to publish the results of the data processing, i.e. the FFT and averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the cluster to run admin functions\n",
    "kafka_admin = KafkaAdminClient(\n",
    "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    ")\n",
    "\n",
    "# define new topic to hold data\n",
    "topic_in = NewTopic(name='chunk_data',\n",
    "                    num_partitions=4, \n",
    "                    replication_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dba41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_out = NewTopic(name='results',\n",
    "                     num_partitions=4, \n",
    "                     replication_factor=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aec9bd",
   "metadata": {},
   "source": [
    "#### 2.2.2. Data Preprocessing <a name=\"kafka_preprocessing\"></a>\n",
    "\n",
    "The size of the files produced by the DAQ is 32 MB, which means that each message handled by Kafka should be 64 MB as we need both the real and imaginary componenets. Unfortunately, the default message size in Kafka is only 1 MB. There are of course ways to circumvent this limit, namely:\n",
    " \n",
    "- at the broker level, changing the *replica.fetch.max.bytes* in the broker settings and increasing the *max.message.bytes* for the topic to the desired value\n",
    "- at the consumer level, increasing the *max.partition.fetch.bytes*, otherwise the consumer will fail to fetch these messages and will get stuck on processing\n",
    "- at the producer level, increasing the *max.request.size* to ensure large messages can be sent\n",
    "\n",
    "While this solution is possible, it is still against the philosophy of Kafka: sending large messages is considered inefficient as they should be huge in number but not in size.\n",
    "\n",
    "We thus decided to first unpack the data into slices and send a pair of real and imaginary slices as a message. Since for each FFT we want *n<sub>bins</sub>* = 3 × 2<sup>10</sup> = 3072 bins and we have a total of 8193 × 2<sup>10</sup> samples per file, the amount of slices to compute FFTs on for each file (and thus of mesages to be sent) is\n",
    "\n",
    "$$n_{slices} = \\cfrac{n_{samples}}{n_{bins}} = \\cfrac{8193 \\times 2^{10} }{3 \\times 2^{10}} = 2731$$\n",
    "\n",
    "The code is actually more flexible, and allows us to send messages with an arbitrary number of slices inside (the maximum being 40, to have messages smaller than 1 MB). For simplicity, we will still only send one slice per message; a more in-depth discussion is present in the [performance tests](#test_kafka) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3820b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "slices_per_msg = 1\n",
    "msg_number = math.ceil(n_slice/slices_per_msg)\n",
    "\n",
    "# read all data from input files\n",
    "real = bytearray(binary_data_real)\n",
    "imag = bytearray(binary_data_imm)\n",
    "\n",
    "# unpack data\n",
    "for f in range(msg_number):\n",
    "    start = 4*n_bins*slices_per_msg*f\n",
    "    end = 4*n_bins*slices_per_msg*(f+1)\n",
    "    if end > 4*n_samples:\n",
    "        end = 4*n_samples\n",
    "        \n",
    "    r_bin = real[start:end]\n",
    "    i_bin = imag[start:end]\n",
    "    msg = r_bin + i_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355bb09",
   "metadata": {},
   "source": [
    "#### 2.2.3. Producer <a name=\"kafka_producer\"></a>\n",
    "\n",
    "Lastly we can initialize the Kafka producer, the one responsible to read data from the files and actually sending it to the correct topic. The message, as descibed before, is given by two consecutives byte arrays containing the real and imaginary slices. The key, instead, contains the number of the file and of the particular slice contained in the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba72111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Kafka producer instance\n",
    "chunk_producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n",
    "\n",
    "def send_chunks(file_paths,dirPath,DAQ_period=5):\n",
    "    \n",
    "    partners = sorted(find_partner(file_paths), key=lambda x: get_number_from_filename(x[0]))\n",
    "    \n",
    "    for couple in partners: \n",
    "        start_time = time.time()\n",
    "            \n",
    "        couple=[dirPath+x for x in couple]\n",
    "        binary_data_real = read_binary_file(couple[0])\n",
    "        binary_data_imm = read_binary_file(couple[1])\n",
    "\n",
    "        real = bytearray(binary_data_real)\n",
    "        imag = bytearray(binary_data_imm)\n",
    "        \n",
    "        file_num=int(couple[0][-9:-4])\n",
    "        \n",
    "        for f in range(msg_number):\n",
    "            start = 4*n_bins*slices_per_msg*f\n",
    "            end = 4*n_bins*slices_per_msg*(f+1)\n",
    "            if end > 4*n_samples:\n",
    "                end = 4*n_samples\n",
    "            r_bin = real[start:end] # one float every 4 bytes\n",
    "            i_bin = imag[start:end]\n",
    "            msg = r_bin + i_bin\n",
    "        \n",
    "            # key = file + bin number\n",
    "            key = (file_num).to_bytes(2, \"big\") + f.to_bytes(2, \"big\")\n",
    "           \n",
    "            #print(\"Sending file\",file_num,\"\\tslice number:\",f+1,end=\"\\r\")\n",
    "            chunk_producer.send(topic = \"chunk_data\",\n",
    "                                key   = key,\n",
    "                                value = msg)\n",
    "            \n",
    "        chunk_producer.flush()  # Flush the producer buffer\n",
    "        \n",
    "        end_time = time.time()\n",
    "        deltat = end_time - start_time\n",
    "        \n",
    "        print(\"                                                                 \")\n",
    "        print(\"File\", file_num,\"completed in \", deltat, \" sec!\")\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        if deltat < DAQ_period:\n",
    "            time.sleep(DAQ_period - deltat)\n",
    "        \n",
    "send_chunks(file_paths,folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e520df",
   "metadata": {},
   "source": [
    "### 2.3. Spark - Distributed Processing <a name=\"spark\"></a>\n",
    "\n",
    "Apache Spark is an open-source unified analytics engine for large-scale data processing. As outlines in the overview, Spark will do the \"heavy lifting\" of the data processing by computing the FFTs for each slice in parallel and averaging them.\n",
    "\n",
    "The first step is to create a spark application. Note that the default number of shuffle partitions has been set to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a4ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"spark://10.67.22.8:7077\") \\\n",
    "        .appName(\"Spark structured streaming application\") \\\n",
    "        .config(\"spark.executor.memory\", \"512m\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df53d89e",
   "metadata": {},
   "source": [
    "#### 2.3.1 Spark Structured Streaming <a name=\"spark_streaming\"></a>\n",
    "\n",
    "In order to deal with the quasi-continuous stream of data from Kafka we will use Spark Structured Streaming. This lets us use the DataFrame API and consider the incoming data as new rows of an unbound table.\n",
    "\n",
    "Given the simplicity and lack of multiple features of the data we could also have chosen to use Spark Streaming, which works by dividing the input data into a sequence micro-batches (DStream) that can be treated as static datasets. Unfortunately, Spark Streaming is considered deprecated, and as such the packages necessary to connect it to Kafka are not present in Spark version 3, which we are currently using.\n",
    "\n",
    "We first create an input (streaming) DataFrame subscribed to the *chunk_data* topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6906b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "    .option(\"kafkaConsumer.pollTimeoutMs\", 30_000) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", n_slice) \\\n",
    "    .option(\"subscribe\", \"chunk_data\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da0459f",
   "metadata": {},
   "source": [
    "Let's explain what some of these options are:\n",
    "- *startingOffset* refers to the start point when a query is started: either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition.\n",
    "- *kafkaConsumer.pollTimeoutMs* is the timeout in milliseconds to poll data from Kafka in executors.\n",
    "- *maxOffsetsPerTrigger* is the rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume.\n",
    "\n",
    "Note that *maxOffsetsPerTrigger* is equal to *n<sub>slices</sub>* (2731), which means that we are never going to deal with batches bigger than a single file. This does not mean, however, that the content of each file will always be processed together, as it is still possible to have batches of smaller size. This last case is not a problem, though, as the division in files depends only on the DAQ system and does not reflect any underlying physics.\n",
    "\n",
    "The input DataFrame from Kafka has the following schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ccfcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- value: binary (nullable = true)\n",
    " |-- topic: string (nullable = true)\n",
    " |-- partition: integer (nullable = true)\n",
    " |-- offset: long (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- timestampType: integer (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87586501",
   "metadata": {},
   "source": [
    "#### 2.3.2 FFT and Averaging <a name=\"spark_fft\"></a>\n",
    "\n",
    "We are only interested in the *key* and *value* pair. We then unpack the *value* column into a list of floats and compute the Fourier Transform. Note that we have to explicitly declare the Fourier Transform as an UDF (User Defined Function) so that it can act on SQL columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50790771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fourier(x):\n",
    "    x = np.array(x)\n",
    "    # take real and imaginary components and getcomplex number  \n",
    "    z = to_complex(x)\n",
    "    \n",
    "    power = np.abs(np.fft.fft(z))**2\n",
    "    FS = fft_bandwidth\n",
    "    norm = n_bins * FS * np.sqrt(2)\n",
    "    normalized_power = power / norm\n",
    "    power_shifted = np.fft.fftshift(normalized_power)\n",
    "    \n",
    "    power_shifted = power_shifted.tolist()\n",
    "    \n",
    "    return(power_shifted)\n",
    "\n",
    "fft_udf = udf(Fourier, ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff8f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your streaming DataFrame and apply transformations\n",
    "streaming_df = inputDF.select('key', 'value')\n",
    "streaming_df = streaming_df.withColumn('float', bytes_to_float32_udf(streaming_df['value']))\n",
    "streaming_df = streaming_df.withColumn('fft', fft_udf(streaming_df['float']))\n",
    "\n",
    "\n",
    "streaming_df = streaming_df.withColumn('file_num', extract_file_num_udf(col('key')))\n",
    "streaming_df = streaming_df.withColumn('indexed_fft', indice_udf(streaming_df['fft'],streaming_df['file_num']) )\n",
    "# Explode the 'indexed_fft' array to separate rows\n",
    "exploded_df = streaming_df.select('key', explode('indexed_fft').alias('indexed_fft'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd3f9c6",
   "metadata": {},
   "source": [
    "We then extract the file number information from the *key* and, after some transformations, obtain a row for each bin with the following schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0bd052",
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- indexed_fft: struct (nullable = true)\n",
    " |    |-- indice: integer (nullable = true)\n",
    " |    |-- x: float (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a45cbd",
   "metadata": {},
   "source": [
    "where *indexed_fft* is a structure containing two elements:\n",
    "- *indice*, the combination of the file number and bin number\n",
    "- *x*, the value fo the FFT in the specific bin for the specific file\n",
    "\n",
    "Finally, we compute the mean and standard deviation of the FFTs for each bin after grouping by the FFT index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'indexed_fft.indice' using dot notation\n",
    "result_df = exploded_df.groupBy(\"indexed_fft.indice\").agg(\n",
    "    mean(\"indexed_fft.x\").alias(\"mean_x\"),\n",
    "    stddev(\"indexed_fft.x\").alias(\"stddev_x\"),\n",
    "    count(\"indexed_fft.x\").alias(\"count_x\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec02af33",
   "metadata": {},
   "source": [
    "#### 2.3.2 Output Message <a name=\"spark_output\"></a>\n",
    "\n",
    "The only step left is to produce a message to send Kafka in the *results* topic. This message is only sent if the whole file has been analyzed, i.e. if all 2731 slices have been used to compute the mean. \n",
    "\n",
    "The message will follow this schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- data: struct (nullable = false)\n",
    " |    |-- indice: integer (nullable = true)\n",
    " |    |-- mean_x: double (nullable = true)\n",
    " |    |-- stddev_x: double (nullable = true)\n",
    " |    |-- count_x: integer (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f63ba",
   "metadata": {},
   "source": [
    "and be sent as Json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e23fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send single json\n",
    "result_json_df = result_df.where(col('count_x')==2731) \\\n",
    "    .select(struct(\"indice\", \"mean_x\", \"stddev_x\",\"count_x\").alias(\"data\"))\n",
    "\n",
    "def send_to_kafka(batch_df, batch_id):\n",
    "    batch_json = batch_df.toJSON().collect()\n",
    "    all_data_json = json.dumps([json.loads(row) for row in batch_json])\n",
    "    \n",
    "    producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n",
    "    producer.send(\"results\", value=all_data_json.encode(\"utf-8\"))\n",
    "    producer.close()\n",
    "\n",
    "# Write the JSON data to Kafka as a single message\n",
    "query = result_json_df.writeStream \\\n",
    "    .trigger(processingTime=\"20 seconds\")\\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(send_to_kafka) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9eefb",
   "metadata": {},
   "source": [
    "### 2.4 Live Plot and Monitoring <a name=\"live_plot\"></a>\n",
    "\n",
    "The only step left is to plot the results and monitor them in real time. To handle live plotting we will use Bokeh. \n",
    "\n",
    "We will show bpth the results for the latest batch sent to Kafka and the cumulative average of all the files processed up until now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42f15e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_source.data['fft'] = (n_cum - 1)/n_cum*np.array(cumulative_source.data['fft']) + \\\n",
    "                                          1/n_cum*np.array(kafka_data['mean'])\n",
    "cum_y = np.array(cumulative_source.data['fft'])\n",
    "cum_sigma = 1/n_cum * np.sqrt(( n_cum -1 )**2 * cum_sigma**2 + sigma**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f59bc0c",
   "metadata": {},
   "source": [
    "The final result looks like this:\n",
    "\n",
    "<video width=\"861\" height=\"505\" \n",
    "       src=\"./Images\\liveplot.webm\"  \n",
    "       controls>\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb41b0",
   "metadata": {},
   "source": [
    "## 3. Performance Tests <a name=\"test\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e52f789",
   "metadata": {},
   "source": [
    "### 3.2. Kafka <a name=\"test_kafka\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0622badd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean = 0.27 +- 0.07\n",
      "Mean = 16.54 +- 0.71\n",
      "Mean = 15.51 +- 0.40\n",
      "Mean = 15.09 +- 0.52\n",
      "Mean = 15.01 +- 0.45\n",
      "Mean = 15.62 +- 0.48\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "no_send = [0.19263148307800293,\n",
    "           0.17882680892944336,\n",
    "           0.19075679779052734,\n",
    "           0.2853243350982666,\n",
    "           0.22602128982543945,\n",
    "           0.40697741508483887,\n",
    "           0.22795748710632324,\n",
    "           0.2530984878540039,\n",
    "           0.23430776596069336,\n",
    "           0.2378981113433838,\n",
    "           0.23514914512634277,\n",
    "           0.4101986885070801,\n",
    "           0.2837076187133789,\n",
    "           0.30994105339050293,\n",
    "           0.3531005382537842,\n",
    "           0.23813819885253906]\n",
    "\n",
    "send_1 = [16.783443927764893,\n",
    "          15.279634714126587,\n",
    "          15.355290651321411,\n",
    "          15.585991859436035,\n",
    "          16.86496877670288,\n",
    "          16.582424640655518,\n",
    "          15.995649099349976,\n",
    "          17.10170030593872,\n",
    "          17.338494777679443,\n",
    "          17.24843192100525,\n",
    "          17.43307375907898,\n",
    "          17.3827862739563,\n",
    "          16.157559394836426,\n",
    "          16.025066375732422,\n",
    "          17.051117658615112,\n",
    "          16.47578740119934]\n",
    "\n",
    "send_10 = [15.814674854278564,\n",
    "           15.83025074005127,\n",
    "           15.502155065536499,\n",
    "           15.249828577041626,\n",
    "           15.124246835708618,\n",
    "           14.991983413696289,\n",
    "           15.013553857803345,\n",
    "           14.93463397026062,\n",
    "           16.170247793197632,\n",
    "           16.067533493041992,\n",
    "           15.825260639190674,\n",
    "           15.469764709472656,\n",
    "           15.535020589828491,\n",
    "           15.316730976104736,\n",
    "           15.207907915115356,\n",
    "           16.054390907287598]\n",
    "\n",
    "send_20 = [15.671099185943604,\n",
    "           15.240856170654297,\n",
    "           14.410763502120972,\n",
    "           14.632994174957275,\n",
    "           14.40667724609375,\n",
    "           14.50796103477478,\n",
    "           15.290517330169678,\n",
    "           15.575354099273682,\n",
    "           15.101133584976196,\n",
    "           14.843635082244873,\n",
    "           14.659634351730347,\n",
    "           15.106323719024658,\n",
    "           14.971516609191895,\n",
    "           16.206547021865845,\n",
    "           15.888291120529175,\n",
    "           14.930995225906372]\n",
    "\n",
    "send_30 = [15.126108884811401,\n",
    "           14.870552062988281,\n",
    "           14.777660131454468,\n",
    "           14.550386905670166,\n",
    "           14.341215372085571,\n",
    "           14.874190092086792,\n",
    "           15.806496143341064,\n",
    "           15.014472961425781,\n",
    "           14.799524307250977,\n",
    "           15.306189060211182,\n",
    "           15.480255603790283,\n",
    "           15.310795783996582,\n",
    "           14.410078048706055,\n",
    "           15.332370281219482,\n",
    "           14.388092756271362,\n",
    "           15.743694305419922]\n",
    "\n",
    "send_40 = [15.711830377578735,\n",
    "           15.316266536712646,\n",
    "           16.28687620162964,\n",
    "           16.40558958053589,\n",
    "           15.795770406723022,\n",
    "           15.743608474731445,\n",
    "           14.552511215209961,\n",
    "           15.992870807647705,\n",
    "           15.652602195739746,\n",
    "           15.885672092437744,\n",
    "           15.579556226730347,\n",
    "           15.706321001052856,\n",
    "           15.725404262542725,\n",
    "           15.670507907867432,\n",
    "           15.145171642303467,\n",
    "           14.695325136184692]\n",
    "\n",
    "times = [no_send, send_1, send_10, send_20, send_30, send_40]\n",
    "for t in times:\n",
    "    t = np.array(t)\n",
    "    m = np.mean(t)\n",
    "    s = np.std(t)\n",
    "    print(f\"Mean = {m:.2f} +- {s:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
