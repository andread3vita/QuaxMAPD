{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8332389",
   "metadata": {},
   "source": [
    "- Andrea De Vita\n",
    "- Enrico Lupi\n",
    "- Manfredi Miranda\n",
    "- Francesco Zane\n",
    "\n",
    "-----------------------\n",
    "\n",
    "# Streaming Processing of the QUAX Experiment Data for the Detection of Galactic Axions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735231d9",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "The axion is a hypothetical particle introduced to solve the strong CP problem of Quantum Chromo Dynamics. It is speculated that axions may also constitute the dark matter (DM) content in our galaxy. The QUAX (QUaerere AXions) experiment aims at detecting this particle by using a copper cavity immersed in a static magnetic field of 8.1 T, cooled down at a working temperature of about 150 mK.\n",
    "\n",
    "The goal of this project is to create a quasi real-time processing chain of the data produced by the QUAX experimental apparatus, and to create a live monitoring system of the detector data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6800bfcf",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction) <br>\n",
    "    1.1. [Experiment](#intro_experiment) <br>\n",
    "    1.2. [Data Structure](#intro_data_structure) <br>\n",
    "    1.3. [Cluster](#intro_cluster) <br>\n",
    "2. [Data Processing](#processing) <br>\n",
    "    2.1. [Pipeline](#pipeline) <br>\n",
    "    2.2. [Kafka](#kafka) <br> \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.1. [Producer](#kafka_producer) <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.2. [Consumer](#kafka_consumer) <br>\n",
    "    2.3. [Spark](#spark) <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.1. [FFT](#spark_fft) <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.1. [Output Message](#spark_output) <br>\n",
    "    2.4. [Live Plot](#live_plot) <br>\n",
    "3. [Performance Tests](#test) <br>\n",
    "    3.1. [Kafka](#test_kafka) <br>\n",
    "    3.2. [Spark](#test_spark) <br>\n",
    "4. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21fe619",
   "metadata": {},
   "source": [
    "## 1. Introduction <a name=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97305fa6",
   "metadata": {},
   "source": [
    "### 1.1. Experiment <a name=\"intro_experiment\"></a>\n",
    "\n",
    "The QUAX experiment aims at the axion detection by using a copper cavity immersed in a static magnetic field of 8.1T, cooled down at a working temperature of about 150mK. The axion is expected to couple with the spin of the electron, interacting with the cavity and inducing a radio-frequency that can be sensed via a Josephson parametric amplifier.\n",
    "\n",
    "For a given configuration of the RF cavity, a scan of the phase of the electromagnetic field is performed to be able to possibly identify a localised excess, a hint of the coupling of an axion with the photon. The data acquisition system of the QUAX experiment generates two streams of digitized reading of the amplifiers, representing the real and imaginary components of the measured phase. To improve the signal over noise ratio, a QUAX data-taking run extends over a long time (up to weeks), repeating the scans over multiple times. Data are saved locally on the DAQ servers in the form of binary files, each corresponding to a multitude of continuous scans performed in the entire frequency range. A single pair of raw files is thus representative of only a few seconds of data taking, but are already including several (thousands) scans. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a2b57",
   "metadata": {},
   "source": [
    "### 1.2. Data Structure <a name=\"intro_data_structure\"></a>\n",
    "\n",
    "The dataset is composed of 2 sets (named duck_i and duck_q respectively) of .dat binary files, each one comprised of a continuous series of ADC readings from the amplifier. Each ADC reading is written in the raw files as a 32 bit floating point value. The ADC readout frequency is 2 × 106 Hz (2 MegaSample per second, or 2MS/s), thus resulting in a raw data throughput of 128 Mbps (16 MB/s). During data taking the readouts are formatted in .dat file such that each file is comprised of 8193 × 210 samples. This results in producing a pair of .dat files (duck_i and duck_q) every 4.2 s.\n",
    "\n",
    "The dataset is provided on a cloud storage s3 bucket hosted on Cloud Veneto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca519fb0",
   "metadata": {},
   "source": [
    "### 1.3. Cluster <a name=\"intro_cluster\"></a>\n",
    "\n",
    "This project has been done on a cluster composed by 4 virtual machines, each with 4 VCPUs with 25 GB disk space and 8 GB RAM each. The virtual machines are hosted on CloudVeneto, an OpenStack-based cloud managed by University of Padova and INFN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ecfb40",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc789d6f",
   "metadata": {},
   "source": [
    "The processing of the raw data is comprised of two phases:\n",
    "1. Run a Fourier transform on each scan to move from the time domain to the frequency domain\n",
    "2. Average (in bins of frequency) all scans in a data-taking run, to extract a single frequency scan\n",
    " \n",
    "This procedure is highly parallelizable, and should be implemented in a quasi-online pipeline for two main reasons:\n",
    "1. Monitoring the scans during the data taking to promptly spot and identify possible issues in the detector setup or instabilities in the condition of the experiment\n",
    "2. Data is continuously produced with a very large rate, and the local storage provided by the DAQ server of the QUAX experiment is not really suited for large-volume and long-lasting datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca25066",
   "metadata": {},
   "source": [
    "### 2.1. Pipeline <a name=\"pipeline\"></a>\n",
    "\n",
    "The data processing pipeline will be implemented as follows:\n",
    "- Each pair of files is unpacked according to their schema and split into scans.\n",
    "- Data is produced to a Kafka topic by a stream-emulator script every 5 seconds to simulate the fixed ADC scanning rate and the fixed size of files written to disk. \n",
    "- The processing of each file runs is performed in a distributed framework using pySpark: for each scan, a FFT is executed in parallel and the results of all FFTs are averaged.\n",
    "- The results are re-injected into a new Kafka topic hosted on the same brokers.\n",
    "- A final consumer performs the plotting, displaying live updates of the scans and continuously updating the entire \"run-wide\" scan using bokeh.\n",
    "\n",
    "The overall pipeline can be thus summarised as:\n",
    "![pipeline schema](Images\\Pipeline_Schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f0bcf",
   "metadata": {},
   "source": [
    "### 2.2. Kafka <a name=\"kafka\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
