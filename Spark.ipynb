{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89408598",
   "metadata": {},
   "source": [
    "# Spark configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e90b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from name import *\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import struct as sstruct\n",
    "\n",
    "from pyspark.sql.functions import udf, col, explode, mean, stddev, count, to_json, struct\n",
    "from pyspark.sql.types import ArrayType, FloatType, IntegerType, StructType, StructField, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from kafka import KafkaProducer,KafkaConsumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed06ce0",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e6907a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://10.67.22.8:7077\")\\\n",
    "    .appName(\"Spark structured streaming application\")\\\n",
    "    .config(\"spark.executor.memory\", \"1000m\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 12)\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23905fd7",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb3e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a byte array into a list of float values\n",
    "def bytes_to_float32_list(bytes_value):\n",
    "    float_list = []\n",
    "    for i in range(0, len(bytes_value), 4): \n",
    "        float_value = sstruct.unpack('f', bytes_value[i:i+4])[0]\n",
    "        float_list.append(float_value)\n",
    "    \n",
    "    return float_list\n",
    "\n",
    "# Function to combine two arrays of length 3072 into complex numbers\n",
    "def to_complex(x):\n",
    "    l = 3072\n",
    "    r = x[:l]\n",
    "    i = x[l:]\n",
    "    c = r + 1j*i\n",
    "    return c\n",
    "\n",
    "# Function to compute the Fourier transform of a given array\n",
    "def Fourie(x):\n",
    "    x = np.array(x)\n",
    "    z = to_complex(x)\n",
    "    \n",
    "    power = np.abs(np.fft.fft(z))**2         # Compute the squared magnitude of the FFT\n",
    "    FS = fft_bandwidth\n",
    "    norm = n_bins * FS * np.sqrt(2)          # Normalization factor\n",
    "    normalized_power = power / norm          # Normalize the power spectrum\n",
    "    power_shifted = np.fft.fftshift(normalized_power) # Shift the power spectrum\n",
    "    \n",
    "    power_shifted = power_shifted.tolist()\n",
    "    \n",
    "    return(power_shifted)\n",
    "\n",
    "# Function to index elements in a list with file numbers\n",
    "def indexing(x,file_num):\n",
    "    k = []\n",
    "    for i in range(len(x)):\n",
    "        add = (f'{file_num}_{i}',x[i])\n",
    "        k.append(add)\n",
    "    return k\n",
    "\n",
    "# Function to extract the file number from a byte array (big-endian short)\n",
    "def extract_file_num(key_bytes):\n",
    "    return sstruct.unpack('>H', key_bytes[:2])[0]  # Unpack from big-endian short\n",
    "\n",
    "# Function to count elements in each batch and print batch size\n",
    "def batches_count(batch_df,batch_id):\n",
    "    batch_count = batch_df.count()\n",
    "    print(f\"Batch {batch_id}: Size = {batch_count}\")\n",
    "\n",
    "# Function to send data to Kafka as JSON messages\n",
    "def send_to_kafka(batch_df, batch_id):\n",
    "    batch_json = batch_df.toJSON().collect()\n",
    "    all_data_json = json.dumps([json.loads(row) for row in batch_json])\n",
    "    \n",
    "    producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n",
    "    producer.send(\"results\", value=all_data_json.encode(\"utf-8\"))\n",
    "    producer.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa6f6e",
   "metadata": {},
   "source": [
    "## Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a779ce2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read streaming data from Kafka into a DataFrame\n",
    "inputDF = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option(\"kafkaConsumer.pollTimeoutMs\", 30_000)\\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 2000)\\\n",
    "    .option(\"subscribe\", \"chunk_data\")\\\n",
    "    .load()\n",
    "\n",
    "\n",
    "# Define user-defined functions (UDFs) and schema\n",
    "bytes_to_float32_udf = udf(bytes_to_float32_list, ArrayType(FloatType()))\n",
    "fft_udf = udf(Fourie, ArrayType(FloatType()))\n",
    "\n",
    "schema = StructType(\n",
    "        [\n",
    "                StructField(\"index\", StringType()),\n",
    "                StructField(\"x\", FloatType())\n",
    "        ]\n",
    ")\n",
    "\n",
    "# Define UDF to extract file number from 'key' column\n",
    "indexing_udf = udf(indexing, ArrayType(schema))\n",
    "extract_file_num_udf = udf(extract_file_num, IntegerType())\n",
    "\n",
    "\n",
    "# Apply UDFs to transform 'value' column in a list of Fourier transformed value\n",
    "streaming_df = inputDF.select('key', 'value')\n",
    "streaming_df = streaming_df.withColumn('float', bytes_to_float32_udf(streaming_df['value']))\n",
    "streaming_df = streaming_df.withColumn('fft', fft_udf(streaming_df['float']))\n",
    "\n",
    "# Extract file numbers from 'key' column\n",
    "streaming_df = streaming_df.withColumn('file_num', extract_file_num_udf(col('key')))\n",
    "\n",
    "# Apply UDF to index 'fft' column by 'file_num'\n",
    "streaming_df = streaming_df.withColumn('indexed_fft', indexing_udf(streaming_df['fft'],streaming_df['file_num']) )\n",
    "\n",
    "# Explode the 'indexed_fft' array to separate rows\n",
    "exploded_df = streaming_df.select('key', explode('indexed_fft').alias('indexed_fft'))\n",
    "\n",
    "# Group by 'indexed_fft.index' and calculate statistics\n",
    "result_df = exploded_df.groupBy(\"indexed_fft.index\").agg(\n",
    "    mean(\"indexed_fft.x\").alias(\"mean_x\"),\n",
    "    stddev(\"indexed_fft.x\").alias(\"stddev_x\"),\n",
    "    count(\"indexed_fft.x\").alias(\"count_x\")\n",
    ")\n",
    "\n",
    "\n",
    "# Code for debugging purpose    \n",
    "# result_json_df.writeStream \\\n",
    "#    .outputMode(\"update\") \\\n",
    "#    .format('console')\\\n",
    "#    .foreachBatch(batches_count)\\\n",
    "#    .start()\\\n",
    "#    .awaitTermination()\n",
    "\n",
    "# Select and structure the data for output as a single JSON message \n",
    "#when the mean is calculated from a full couple of file\n",
    "result_json_df = result_df.where(col('count_x')==2731)\\\n",
    "    .select(struct(\"index\", \"mean_x\", \"stddev_x\",\"count_x\").alias(\"data\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Write the JSON data to Kafka as a single message, triggered every 12 seconds\n",
    "query = result_json_df.writeStream \\\n",
    "    .trigger(processingTime=\"12 seconds\")\\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(send_to_kafka) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4894e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
